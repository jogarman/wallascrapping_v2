# Wallapop Scraper V2 ğŸ”

[![Docker Tested](https://img.shields.io/badge/Docker-Tested%20%E2%9C%85-success?style=flat-square&logo=docker)](https://github.com/jogarman/wallascrapping_v2)
[![Python](https://img.shields.io/badge/Python-3.11-blue?style=flat-square&logo=python)](https://www.python.org/)
[![Selenium](https://img.shields.io/badge/Selenium-Automated-green?style=flat-square&logo=selenium)](https://www.selenium.dev/)
[![Apify](https://img.shields.io/badge/Apify-Ready-orange?style=flat-square&logo=apify)](https://apify.com/)

Scraper automatizado para Wallapop con pipeline completo de procesamiento de datos.

## âœ¨ CaracterÃ­sticas

- ğŸ¤– **Scraping automatizado** con Selenium
- ğŸ³ **Docker containerizado** para fÃ¡cil deployment
- ğŸ”„ **Pipeline de 5 pasos** para procesar y filtrar datos
- ğŸ§  **Enriquecimiento con IA** usando Google Gemini
- â˜ï¸ **Listo para Apify** - Deploy directo a la nube
- ğŸ” **Anti-detecciÃ³n** con User-Agent rotaciÃ³n y medidas anti-bot

## ğŸš€ Quick Start

### OpciÃ³n 1: Docker (Recomendado)

```bash
# Construir imagen
docker build -t wallascrap:latest .

# Ejecutar
docker run --rm \
  -e GOOGLE_API_KEY="your_api_key" \
  -e SEARCH_TERM="iphone" \
  -e HEADLESS=true \
  -v "$(pwd)/scrapping_outputs:/usr/src/app/scrapping_outputs" \
  -v "$(pwd)/data:/usr/src/app/data" \
  wallascrap:latest
```

### OpciÃ³n 2: Local con UV

```bash
# Instalar dependencias
uv sync

# Ejecutar
uv run python -m src.main
```

## ğŸ“Š Pipeline

El scraper ejecuta 5 pasos automÃ¡ticamente:

1. **Scraper** â†’ Extrae items de Wallapop
2. **Filtro Inicial** â†’ Filtra por precio y condiciones
3. **LÃ³gica de Negocio** â†’ Aplica blacklist/whitelist
4. **Enriquecimiento Gemini** â†’ Analiza con IA
5. **FinalizaciÃ³n** â†’ Genera output final

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno

| Variable | DescripciÃ³n | Requerido |
|----------|-------------|-----------|
| `GOOGLE_API_KEY` | API key de Google Gemini | âœ… |
| `SEARCH_TERM` | TÃ©rmino de bÃºsqueda | âŒ |
| `HEADLESS` | Modo headless (`true`/`false`) | âŒ |
| `SCROLLS` | NÃºmero de scrolls | âŒ |

### Archivos de ConfiguraciÃ³n

- `config/general_scrap_config.json` - ConfiguraciÃ³n del scraper
- `input_schema.json` - Schema de inputs para Apify
- `elements_to_scrap.json` - Selectores CSS

## ğŸ“¦ Output

El scraper genera:

- `scrapping_outputs/orchestrator_*.log` - Logs de ejecuciÃ³n
- `data/step1/raw_*.csv` - Datos crudos scrapeados
- `data/step2_inc/filtered_*.csv` - Items incluidos
- `data/step2_exc/excluded_*.csv` - Items excluidos
- Screenshots de debug en caso de error

## âœ… Testing

**Ãšltima prueba:** 17 Enero 2026

```
âœ… Imagen Docker construida: 2.51GB
âœ… Items scrapeados: 200
âœ… Pipeline completado: 5/5 steps
âœ… Exit code: 0 (Success)
```

## ğŸŒ Deploy a Apify

```bash
# Instalar CLI
npm install -g apify-cli

# Login
apify login

# Push
apify push
```

O conecta tu repositorio de GitHub en [Apify Console](https://console.apify.com) para deploys automÃ¡ticos.

## ğŸ“ Licencia

MIT

## ğŸ‘¤ Autor

[@jogarman](https://github.com/jogarman)
